{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da5b7875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark import SparkContext\n",
    "from pyspark.conf import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5db4251a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/apache-spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/fernblade/.ivy2/cache\n",
      "The jars for the packages stored in: /home/fernblade/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9bf92a5c-de3a-416e-8a32-9995ce34c9bd;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector;10.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.5.1 in central\n",
      "\t[4.5.1] org.mongodb#mongodb-driver-sync;[4.5.0,4.5.99)\n",
      "\tfound org.mongodb#bson;4.5.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.5.1 in central\n",
      ":: resolution report :: resolve 1697ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.5.1 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.5.1 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.5.1 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector;10.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   1   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-9bf92a5c-de3a-416e-8a32-9995ce34c9bd\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/4ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/26 16:16:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf()\n",
    "\n",
    "    # Download mongo-spark-connector and its dependencies.\n",
    "conf.set(\"spark.jars.packages\",\n",
    "             \"org.mongodb.spark:mongo-spark-connector:10.0.1\")\n",
    "mongo_conn = f\"mongodb://localhost:27017/fraud_detection.my_collection\"\n",
    "    # Set up read connection :\n",
    "conf.set(\"spark.mongodb.read.connection.uri\", mongo_conn)\n",
    "conf.set(\"spark.mongodb.write.database\", \"fraud_detecion\")\n",
    "conf.set(\"spark.mongodb.write.collection\", \"my_collection\")\n",
    "\n",
    "    # Set up write connection\n",
    "conf.set(\"spark.mongodb.read.connection.uri\", mongo_conn)\n",
    "conf.set(\"spark.mongodb.write.database\", \"fraud_detecion\")\n",
    "conf.set(\"spark.mongodb.write.collection\", \"my_collection\")\n",
    "    # If you need to update instead of inserting :\n",
    "conf.set(\"spark.mongodb.write.operationType\", \"update\")\n",
    "SparkContext(conf=conf)\n",
    "\n",
    "spark= SparkSession \\\n",
    "        .builder \\\n",
    "        .appName('myApp') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a967a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"mongodb\").load()\n",
    "# df.repartition(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8157c201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "150bb1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# spark = SparkSession.builder.appName(\"FraudDetection\").getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "df1 = spark.read.csv(\"./HI-Small_Trans.csv\", header=True, inferSchema=True)\n",
    "df2 = spark.read.csv(\"./LI-Small_Trans.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# # Perform join operation\n",
    "df = df1.union(df2)\n",
    "# # Write data to Cassandra\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc89d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c622597",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12002394"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62345016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Timestamp='2022/09/01 00:20', From Bank=10, From_Account='8000EBD30', To Bank=10, To_Account='8000EBD30', Amount Received=3697.34, Receiving Currency='US Dollar', Amount Paid=3697.34, Payment Currency='US Dollar', Payment Format='Reinvestment', Is Laundering=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d65f5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "df = df.na.drop()\n",
    "\n",
    "# Fill missing values with a default value\n",
    "df = df.na.fill({\"Amount Received\": 0, \"Amount Paid\": 0})\n",
    "\n",
    "# Replace missing values with a specified value\n",
    "df = df.na.replace([\"Reinvestment\"], [\"Unknown\"], \"Payment Format\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3805fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "df = df.withColumn(\"Timestamp\", to_timestamp(col(\"Timestamp\"), \"yyyy/MM/dd HH:mm\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "989650b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Timestamp=datetime.datetime(2022, 9, 1, 0, 20), From Bank=10, From_Account='8000EBD30', To Bank=10, To_Account='8000EBD30', Amount Received=3697.34, Receiving Currency='US Dollar', Amount Paid=3697.34, Payment Currency='US Dollar', Payment Format='Unknown', Is Laundering=0),\n",
       " Row(Timestamp=datetime.datetime(2022, 9, 1, 0, 20), From Bank=3208, From_Account='8000F4580', To Bank=1, To_Account='8000F5340', Amount Received=0.01, Receiving Currency='US Dollar', Amount Paid=0.01, Payment Currency='US Dollar', Payment Format='Cheque', Is Laundering=0),\n",
       " Row(Timestamp=datetime.datetime(2022, 9, 1, 0, 0), From Bank=3209, From_Account='8000F4670', To Bank=3209, To_Account='8000F4670', Amount Received=14675.57, Receiving Currency='US Dollar', Amount Paid=14675.57, Payment Currency='US Dollar', Payment Format='Unknown', Is Laundering=0),\n",
       " Row(Timestamp=datetime.datetime(2022, 9, 1, 0, 2), From Bank=12, From_Account='8000F5030', To Bank=12, To_Account='8000F5030', Amount Received=2806.97, Receiving Currency='US Dollar', Amount Paid=2806.97, Payment Currency='US Dollar', Payment Format='Unknown', Is Laundering=0),\n",
       " Row(Timestamp=datetime.datetime(2022, 9, 1, 0, 6), From Bank=10, From_Account='8000F5200', To Bank=10, To_Account='8000F5200', Amount Received=36682.97, Receiving Currency='US Dollar', Amount Paid=36682.97, Payment Currency='US Dollar', Payment Format='Unknown', Is Laundering=0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b084153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Convert Amount Received and Amount Paid columns to DoubleType\n",
    "df = df.withColumn(\"Amount Received\", col(\"Amount Received\").cast(DoubleType()))\n",
    "df = df.withColumn(\"Amount Paid\", col(\"Amount Paid\").cast(DoubleType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fad2fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b672649d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour, dayofmonth, month, year\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Convert Timestamp column to TimestampType\n",
    "df = df.withColumn(\"Timestamp\", df[\"Timestamp\"].cast(\"timestamp\"))\n",
    "\n",
    "# Extract Hour, Day, Month, and Year from Timestamp\n",
    "df = df.withColumn(\"Hour\", hour(df[\"Timestamp\"]).cast(IntegerType()))\n",
    "df = df.withColumn(\"Day\", dayofmonth(df[\"Timestamp\"]).cast(IntegerType()))\n",
    "df = df.withColumn(\"Month\", month(df[\"Timestamp\"]).cast(IntegerType()))\n",
    "df = df.withColumn(\"Year\", year(df[\"Timestamp\"]).cast(IntegerType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfb6327a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Create StringIndexer for Payment Format\n",
    "payment_format_indexer = StringIndexer(inputCol=\"Payment Format\", outputCol=\"Payment Format Index\")\n",
    "\n",
    "# Fit and transform the DataFrame\n",
    "df = payment_format_indexer.fit(df).transform(df)\n",
    "\n",
    "# Drop the original Payment Format column\n",
    "df = df.drop(\"Payment Format\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcc7cfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "receiving_currency_indexer = StringIndexer(inputCol=\"Receiving Currency\", outputCol=\"Receiving Currency Index\")\n",
    "\n",
    "# Create StringIndexer for Payment Currency\n",
    "payment_currency_indexer = StringIndexer(inputCol=\"Payment Currency\", outputCol=\"Payment Currency Index\")\n",
    "\n",
    "# Fit and transform the DataFrame\n",
    "df = receiving_currency_indexer.fit(df).transform(df)\n",
    "df = payment_currency_indexer.fit(df).transform(df)\n",
    "\n",
    "# Drop the original Receiving Currency and Payment Currency columns\n",
    "df = df.drop(\"Receiving Currency\", \"Payment Currency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5d97720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Timestamp=datetime.datetime(2022, 9, 1, 0, 20), From Bank=10, From_Account='8000EBD30', To Bank=10, To_Account='8000EBD30', Amount Received=3697.34, Amount Paid=3697.34, Is Laundering=0, Hour=0, Day=1, Month=9, Year=2022, Payment Format Index=4.0, Receiving Currency Index=0.0, Payment Currency Index=0.0),\n",
       " Row(Timestamp=datetime.datetime(2022, 9, 1, 0, 20), From Bank=3208, From_Account='8000F4580', To Bank=1, To_Account='8000F5340', Amount Received=0.01, Amount Paid=0.01, Is Laundering=0, Hour=0, Day=1, Month=9, Year=2022, Payment Format Index=0.0, Receiving Currency Index=0.0, Payment Currency Index=0.0),\n",
       " Row(Timestamp=datetime.datetime(2022, 9, 1, 0, 0), From Bank=3209, From_Account='8000F4670', To Bank=3209, To_Account='8000F4670', Amount Received=14675.57, Amount Paid=14675.57, Is Laundering=0, Hour=0, Day=1, Month=9, Year=2022, Payment Format Index=4.0, Receiving Currency Index=0.0, Payment Currency Index=0.0),\n",
       " Row(Timestamp=datetime.datetime(2022, 9, 1, 0, 2), From Bank=12, From_Account='8000F5030', To Bank=12, To_Account='8000F5030', Amount Received=2806.97, Amount Paid=2806.97, Is Laundering=0, Hour=0, Day=1, Month=9, Year=2022, Payment Format Index=4.0, Receiving Currency Index=0.0, Payment Currency Index=0.0),\n",
       " Row(Timestamp=datetime.datetime(2022, 9, 1, 0, 6), From Bank=10, From_Account='8000F5200', To Bank=10, To_Account='8000F5200', Amount Received=36682.97, Amount Paid=36682.97, Is Laundering=0, Hour=0, Day=1, Month=9, Year=2022, Payment Format Index=4.0, Receiving Currency Index=0.0, Payment Currency Index=0.0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "603b184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(\"TimeStamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c86512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(\"From_Account\",\"To_Account\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1216c444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[From Bank: int, To Bank: int, Amount Received: double, Amount Paid: double, Is Laundering: int, Hour: int, Day: int, Month: int, Year: int, Payment Format Index: double, Receiving Currency Index: double, Payment Currency Index: double]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8807a78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Extract X_train, y_train, X_test, y_test from the DataFrames\n",
    "X_train = train_data.drop(\"Is Laundering\")  # Drop the 'Is Laundering' column to get features\n",
    "y_train = train_data.select(\"Is Laundering\")  # Select only the 'Is Laundering' column as the label\n",
    "X_test = test_data.drop(\"Is Laundering\")\n",
    "y_test = test_data.select(\"Is Laundering\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8926b076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:>                                                        (0 + 8) / 16]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Assemble features into a vector column\n",
    "assembler = VectorAssembler(inputCols=X_train.columns, outputCol=\"features\")\n",
    "\n",
    "# Create a Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"Is Laundering\")\n",
    "\n",
    "# Create a pipeline with the assembler and the Decision Tree classifier\n",
    "pipeline = Pipeline(stages=[assembler, dt])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c472b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.types import IntegerType\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Is Laundering\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy: {:.4f}\".format(accuracy))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e3d507",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
